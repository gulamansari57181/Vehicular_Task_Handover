{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2509a6-8778-4547-839d-b6b731ce515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Hyperparameters and constants\n",
    "RSU_COVERAGE = 800          # meters\n",
    "HANDOVER_AREA = 300         # meters\n",
    "TASK_SIZE_RANGE = (1, 10)   # MB\n",
    "CPU_CYCLES_PER_MB = 500     # cycles per MB\n",
    "VEHICLE_CPU = 5000          # MHz\n",
    "COOPERATIVE_VEHICLE_CPU = (10000, 50000)  # MHz range\n",
    "RSU_CPU = (50000, 100000)   # MHz range\n",
    "V2R_BANDWIDTH = 10          # MHz\n",
    "V2V_BANDWIDTH = 5           # MHz\n",
    "BANDWIDTH_COST_RSU = 1      # Cost per MHz for RSU\n",
    "BANDWIDTH_COST_V2V = 1.5     # Cost per MHz for V2V\n",
    "LEARNING_RATE = 0.0001      \n",
    "GAMMA = 0.99                \n",
    "EPSILON_DECAY = 0.999       \n",
    "EPSILON_MIN = 0.1         \n",
    "TAU = 0.01                \n",
    "ALPHA = 1.5               \n",
    "BETA = 1.5                \n",
    "POWER_LOCAL = 1.0         \n",
    "POWER_RSU = 0.8           \n",
    "POWER_V2V = 1.2           \n",
    "\n",
    "# Global tracking variables\n",
    "global_total_latency_local = 0\n",
    "global_total_latency_rsu = 0\n",
    "global_total_latency_v2v = 0\n",
    "global_total_energy_local = 0\n",
    "global_total_energy_rsu = 0\n",
    "global_total_energy_v2v = 0\n",
    "global_count_local = 0\n",
    "global_count_rsu = 0\n",
    "global_count_v2v = 0\n",
    "episode_rewards = []\n",
    "\n",
    "# -----------------------------\n",
    "# Utility Functions\n",
    "# -----------------------------\n",
    "def transmission_delay(task_size, bandwidth):\n",
    "    return task_size * 8 / bandwidth\n",
    "\n",
    "def compute_handover_cost(vehicle, rsu1, rsu2):\n",
    "    if vehicle.x >= RSU_COVERAGE:\n",
    "        return (vehicle.task_size * 8) / V2R_BANDWIDTH\n",
    "    return 0\n",
    "\n",
    "# -----------------------------\n",
    "# Trajectory Prediction Functions\n",
    "# -----------------------------\n",
    "def load_trajectory_data():\n",
    "    pred_data = np.load(\"batch_data/test_predictions_full_300.npz\", allow_pickle=True)\n",
    "    true_positions = pred_data['true']\n",
    "    predicted_positions = pred_data['predicted']\n",
    "    predicted_positions_dict = {i: predicted_positions[i] for i in range(len(predicted_positions))}\n",
    "    return true_positions, predicted_positions, predicted_positions_dict\n",
    "\n",
    "def train_trajectory_model(true_positions, predicted_positions):\n",
    "    X, y_lat, y_lon = [], [], []\n",
    "    for i in range(len(true_positions)):\n",
    "        X.append(true_positions[i, :4])  # [x, y, velocity, direction]\n",
    "        y_lat.append(predicted_positions[i, 0])\n",
    "        y_lon.append(predicted_positions[i, 1])\n",
    "    X = np.array(X)\n",
    "    y_lat = np.array(y_lat)\n",
    "    y_lon = np.array(y_lon)\n",
    "    model_lat = LGBMRegressor()\n",
    "    model_lon = LGBMRegressor()\n",
    "    model_lat.fit(X, y_lat)\n",
    "    model_lon.fit(X, y_lon)\n",
    "    return model_lat, model_lon\n",
    "\n",
    "def predict_trajectory(vehicle, model_lat, model_lon):\n",
    "    traj_lat = model_lat.predict([[vehicle.x, vehicle.y, vehicle.velocity, vehicle.direction]])\n",
    "    traj_lon = model_lon.predict([[vehicle.x, vehicle.y, vehicle.velocity, vehicle.direction]])\n",
    "    return traj_lat[0], traj_lon[0]\n",
    "\n",
    "def select_cooperative_vehicle(vehicle, cooperative_vehicles, model_lat, model_lon):\n",
    "    pred_lat, pred_lon = predict_trajectory(vehicle, model_lat, model_lon)\n",
    "    selected_vehicle = None\n",
    "    min_distance = float('inf')\n",
    "    for coop_vehicle in cooperative_vehicles:\n",
    "        coop_lat, coop_lon = predict_trajectory(coop_vehicle, model_lat, model_lon)\n",
    "        distance = np.linalg.norm(np.array([pred_lat, pred_lon]) - np.array([coop_lat, coop_lon]))\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            selected_vehicle = coop_vehicle\n",
    "    return selected_vehicle\n",
    "\n",
    "# -----------------------------\n",
    "# DDQN Agent Implementation\n",
    "# -----------------------------\n",
    "class DDQNAgent:\n",
    "    def __init__(self, input_dim=9, action_dim=3):\n",
    "        self.input_dim = input_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.online_network = self.build_network()\n",
    "        self.target_network = self.build_network()\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.epsilon = 1.0\n",
    "        self.update_target_network(hard=True)\n",
    "\n",
    "    def build_network(self):\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=(self.input_dim,)),\n",
    "            tf.keras.layers.Dense(128, activation='relu'),\n",
    "            tf.keras.layers.Dense(self.action_dim, activation=None)\n",
    "        ])\n",
    "        return model\n",
    "\n",
    "    def update_target_network(self, hard=False):\n",
    "        if hard:\n",
    "            # Hard copy all weights from online to target network\n",
    "            self.target_network.set_weights(self.online_network.get_weights())\n",
    "        else:\n",
    "            # Soft update using factor TAU\n",
    "            online_weights = self.online_network.get_weights()\n",
    "            target_weights = self.target_network.get_weights()\n",
    "            new_weights = []\n",
    "            for ow, tw in zip(online_weights, target_weights):\n",
    "                new_weights.append(TAU * ow + (1 - TAU) * tw)\n",
    "            self.target_network.set_weights(new_weights)\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        state = np.array([state])\n",
    "        q_values = self.online_network(state)\n",
    "        return np.argmax(q_values.numpy()[0])\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.memory) < 32:\n",
    "            return\n",
    "        batch = random.sample(self.memory, 32)\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for s, a, r, ns, d in batch:\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(ns)\n",
    "            dones.append(d)\n",
    "        states = np.array(states)\n",
    "        next_states = np.array(next_states)\n",
    "        # Compute Q-values from online network for next_states to select best action\n",
    "        next_q_online = self.online_network(next_states)\n",
    "        best_actions = np.argmax(next_q_online.numpy(), axis=1)\n",
    "        # Evaluate Q-values of best actions using target network\n",
    "        next_q_target = self.target_network(next_states).numpy()\n",
    "        target_q = []\n",
    "        for i in range(len(batch)):\n",
    "            if dones[i]:\n",
    "                target_q.append(rewards[i])\n",
    "            else:\n",
    "                target_q.append(rewards[i] + GAMMA * next_q_target[i][best_actions[i]])\n",
    "        target_q = np.array(target_q)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.online_network(states)\n",
    "            q_action = tf.reduce_sum(q_values * tf.one_hot(actions, self.action_dim), axis=1)\n",
    "            loss = tf.reduce_mean(tf.square(target_q - q_action))\n",
    "        grads = tape.gradient(loss, self.online_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.online_network.trainable_variables))\n",
    "        # Soft update target network\n",
    "        self.update_target_network()\n",
    "        \n",
    "# -----------------------------\n",
    "# Classes for Vehicles and RSU \n",
    "# -----------------------------\n",
    "class MissionVehicle:\n",
    "    def __init__(self, vehicle_id):\n",
    "        self.vehicle_id = vehicle_id\n",
    "        self.x, self.y = true_positions[vehicle_id, :2]\n",
    "        self.velocity, self.direction = true_positions[vehicle_id, 2:]\n",
    "        self.cpu = VEHICLE_CPU\n",
    "        self.task_size = random.uniform(*TASK_SIZE_RANGE)\n",
    "        self.task_cycles = self.task_size * CPU_CYCLES_PER_MB\n",
    "\n",
    "    def update_position(self, predicted_positions_dict):\n",
    "        if self.vehicle_id in predicted_positions_dict:\n",
    "            predicted_values = predicted_positions_dict[self.vehicle_id]\n",
    "            self.x, self.y, self.velocity, self.direction = predicted_values\n",
    "\n",
    "class CooperativeVehicle:\n",
    "    def __init__(self, vehicle_id):\n",
    "        self.vehicle_id = vehicle_id\n",
    "        self.x, self.y = true_positions[vehicle_id, :2]\n",
    "        self.cpu = random.randint(*COOPERATIVE_VEHICLE_CPU)\n",
    "        self.velocity = true_positions[vehicle_id, 2]\n",
    "        self.direction = true_positions[vehicle_id, 3]\n",
    "\n",
    "    def update_position(self, predicted_positions_dict):\n",
    "        if self.vehicle_id in predicted_positions_dict:\n",
    "            predicted_values = predicted_positions_dict[self.vehicle_id]\n",
    "            self.x, self.y, self.velocity, self.direction = predicted_values\n",
    "\n",
    "class RSU:\n",
    "    def __init__(self, location):\n",
    "        self.location = location\n",
    "        self.cpu = random.randint(*RSU_CPU)\n",
    "        self.bandwidth = random.randint(1000, 5000)\n",
    "\n",
    "    def update_resources(self):\n",
    "        self.cpu = random.randint(*RSU_CPU)\n",
    "        self.bandwidth = random.randint(1000, 5000)\n",
    "\n",
    "# -----------------------------\n",
    "# Load Data and Initialize Global Objects\n",
    "# -----------------------------\n",
    "true_positions, predicted_positions, predicted_positions_dict = load_trajectory_data()\n",
    "model_lat, model_lon = train_trajectory_model(true_positions, predicted_positions)\n",
    "mission_vehicles = [MissionVehicle(i) for i in range(50)]\n",
    "cooperative_vehicles = [CooperativeVehicle(j) for j in range(10)]\n",
    "rsus = [RSU(location) for location in [1000, 2000]]\n",
    "ddqn = DDQNAgent()\n",
    "# Offloading counts dictionary for DDQN simulation\n",
    "ddqn_offloading_counts = {\"Local\": 0, \"RSU\": 0, \"V2V\": 0}\n",
    "\n",
    "def run_ddqn_simulation():\n",
    "    global global_total_latency_local, global_total_latency_rsu, global_total_latency_v2v\n",
    "    global global_total_energy_local, global_total_energy_rsu, global_total_energy_v2v\n",
    "    global global_count_local, global_count_rsu, global_count_v2v\n",
    "\n",
    "    ddqn_episode_rewards = []\n",
    "    for episode in range(1000):\n",
    "        total_reward = 0\n",
    "        for mv in mission_vehicles:\n",
    "            mv.update_position(predicted_positions_dict)\n",
    "            # State vector\n",
    "            state = [mv.task_size, mv.task_cycles, mv.x, mv.y, mv.velocity, mv.direction, mv.cpu, V2R_BANDWIDTH, V2V_BANDWIDTH]\n",
    "            action = ddqn.act(state)\n",
    "            comm_revenue = mv.task_size * 10\n",
    "            comp_revenue = mv.task_cycles / 1000\n",
    "            if action == 0:  # Local processing\n",
    "                delay = mv.task_cycles / mv.cpu\n",
    "                reward = ALPHA * (comp_revenue - delay)\n",
    "                ddqn_offloading_counts[\"Local\"] += 1\n",
    "                latency_local = delay\n",
    "                energy_local = latency_local * POWER_LOCAL\n",
    "                global_total_latency_local += latency_local\n",
    "                global_total_energy_local += energy_local\n",
    "                global_count_local += 1\n",
    "            elif action == 1:  # RSU Offloading\n",
    "                selected_rsu = min(rsus, key=lambda r: abs(mv.x - r.location))\n",
    "                delay_comm = transmission_delay(mv.task_size, V2R_BANDWIDTH)\n",
    "                delay_comp = mv.task_cycles / selected_rsu.cpu\n",
    "                cost = BANDWIDTH_COST_RSU * V2R_BANDWIDTH\n",
    "                reward = ALPHA * (comm_revenue - delay_comm) + BETA * (comp_revenue - delay_comp) - cost\n",
    "                ddqn_offloading_counts[\"RSU\"] += 1\n",
    "                latency_rsu = delay_comm + delay_comp\n",
    "                energy_rsu = latency_rsu * POWER_RSU\n",
    "                global_total_latency_rsu += latency_rsu\n",
    "                global_total_energy_rsu += energy_rsu\n",
    "                global_count_rsu += 1\n",
    "            else:  # V2V Offloading\n",
    "                selected_cv = select_cooperative_vehicle(mv, cooperative_vehicles, model_lat, model_lon)\n",
    "                if selected_cv is None:\n",
    "                    latency = mv.task_cycles / mv.cpu\n",
    "                    energy = latency * POWER_LOCAL\n",
    "                else:\n",
    "                    delay_comm = transmission_delay(mv.task_size, V2V_BANDWIDTH)\n",
    "                    delay_comp = mv.task_cycles / selected_cv.cpu\n",
    "                    cost = BANDWIDTH_COST_V2V * V2V_BANDWIDTH\n",
    "                    reward = ALPHA * (comm_revenue - delay_comm) + BETA * (comp_revenue - delay_comp) - cost\n",
    "                    ddqn_offloading_counts[\"V2V\"] += 1\n",
    "                    latency_v2v = delay_comm + delay_comp\n",
    "                    energy_v2v = latency_v2v * POWER_V2V\n",
    "                    global_total_latency_v2v += latency_v2v\n",
    "                    global_total_energy_v2v += energy_v2v\n",
    "                    global_count_v2v += 1\n",
    "            total_reward += reward\n",
    "            next_state = state\n",
    "            ddqn.store_experience(state, action, reward, next_state, done=False)\n",
    "        if ddqn.epsilon > EPSILON_MIN:\n",
    "            ddqn.epsilon = max(ddqn.epsilon * EPSILON_DECAY, EPSILON_MIN)\n",
    "        ddqn_episode_rewards.append(total_reward)\n",
    "        ddqn.train()\n",
    "        if episode % 50 == 0:\n",
    "            print(f\"DDQN Episode {episode} - Total Reward: {total_reward:.2f}\")\n",
    "    return ddqn, ddqn_episode_rewards\n",
    "\n",
    "# Visualization functions (you can reuse the same functions as before)\n",
    "def plot_training_rewards(episode_rewards):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(range(len(episode_rewards)), episode_rewards, marker='o', linestyle='-', label=\"Training Reward\")\n",
    "    plt.title(\"DDQN Training: Reward Over Episodes\")\n",
    "    plt.xlabel(\"Episodes\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_offloading_distribution(offloading_counts):\n",
    "    labels = list(offloading_counts.keys())\n",
    "    sizes = list(offloading_counts.values())\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=['gold', 'lightblue', 'lightgreen'])\n",
    "    plt.title(\"DDQN Offloading Decision Distribution\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_latency_distribution():\n",
    "    avg_latency_local = global_total_latency_local / global_count_local if global_count_local > 0 else 0\n",
    "    avg_latency_rsu = global_total_latency_rsu / global_count_rsu if global_count_rsu > 0 else 0\n",
    "    avg_latency_v2v = global_total_latency_v2v / global_count_v2v if global_count_v2v > 0 else 0\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.bar(['Local','RSU','V2V'], [avg_latency_local, avg_latency_rsu, avg_latency_v2v],\n",
    "            color=['gold','lightblue','lightgreen'])\n",
    "    plt.title(\"DDQN Average Latency by Offloading Option\")\n",
    "    plt.ylabel(\"Latency (s)\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_energy_consumption():\n",
    "    avg_energy_local = global_total_energy_local / global_count_local if global_count_local > 0 else 0\n",
    "    avg_energy_rsu = global_total_energy_rsu / global_count_rsu if global_count_rsu > 0 else 0\n",
    "    avg_energy_v2v = global_total_energy_v2v / global_count_v2v if global_count_v2v > 0 else 0\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.bar(['Local','RSU','V2V'], [avg_energy_local, avg_energy_rsu, avg_energy_v2v],\n",
    "            color=['gold','lightblue','lightgreen'])\n",
    "    plt.title(\"DDQN Average Energy Consumption by Offloading Option\")\n",
    "    plt.ylabel(\"Energy (Joules)\")\n",
    "    plt.show()\n",
    "\n",
    "# Run DDQN simulation and plot results\n",
    "ddqn_agent, ddqn_episode_rewards = run_ddqn_simulation()\n",
    "plot_training_rewards(ddqn_episode_rewards)\n",
    "plot_offloading_distribution(ddqn_offloading_counts)\n",
    "plot_latency_distribution()\n",
    "plot_energy_consumption()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
